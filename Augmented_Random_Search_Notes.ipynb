{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Random Search (ARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ARS is a Reinforcement Learning algorithm based on evolution strategies\n",
    "    - the difference is that ARS uses a simple linear policy(perceptron) instead of parallelized deep neural networks\n",
    "- ARS aims to optimize a policy(mapping from state to action)\n",
    "- the input of the policy is the environment state at time $t$\n",
    "    - in the case of Half-Cheetah environment, the state is an encoded vector including:\n",
    "        - coordinates of the angular points\n",
    "        - the angle of rotation around the rotors\n",
    "        - velocity and so on\n",
    "- the output of the policy is a group of actions\n",
    "    - these actions are represented as a vector of continuous values\n",
    "    - in the case of Half-Cheetah, this are the forces applied to the \"muscles\" of the cheetah\n",
    "- that means the input and output are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ARS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Initialization\n",
    "\n",
    "- init all the weights of our linear policy(perceptron) at $t_0$ with zero\n",
    "\n",
    "2.) Applying perturbations to the weights\n",
    "- we apply some very little perturbations $\\delta_{i,j}$ to our weight matrix\n",
    "- $\\delta_{i,j}$ is sampled from a gaussian distribution $\\mathcal{N(0, \\sigma)}$\n",
    "    - $\\sigma$ is the standard deviation\n",
    "- overall we sample one specific matrix of little perturbations $\\Delta_k$ with some values(close to 0) $\\delta_{i,j}$ taken from a gaussian distribution $\\mathcal{N(0, \\sigma)}$ and update our weights by adding and subtracting $\\Delta_k$\n",
    "- as result we got 2 new slightly modified weight matrices\n",
    "\n",
    "    weights_plus_pos_delta = weights + $\\Delta_k$ <br>\n",
    "    weights_plus_neg_delta = weights - $\\Delta_k$\n",
    "    \n",
    "    \n",
    "- in the final algorithm, we do this $n$ times, resulting in 2$n$ different weight matrices\n",
    "- that means we update our weights in different directions \n",
    "- we want to find the changes that lead to the highest rewards\n",
    "\n",
    "Why we need the positive_delta and negative_delta pairs?\n",
    "- once we figure out the directions that increase the most the rewards(by merely getting the\n",
    "accumulated reward over the full episode for each direction and then sorting them by the highest obtained),\n",
    "we will do one step of gradient descent to update the weights in these best directions\n",
    "- the problem is, to apply gradient descent we would need a reward function of the weights which we differentiate with respect to the weights to do one step of gradient descent to update the weights\n",
    "- but we don't have an explicit expression of the reward with respect to the weights\n",
    "- that means we can't compute the gradient directly instead we will approximate it\n",
    "- the approximation method is called the method of finite differences, and for this, we need the delta pair\n",
    "\n",
    "3.) Approximated Gradient Descent with the Method of Finite Differences\n",
    "\n",
    "- the value of each perturbation $\\delta$ is a tiny number close to zero\n",
    "- we could use the reward collected with weights_plus_pos_delta and the reward collected by weights_plus_neg_delta to approximate the gradient\n",
    "\n",
    "$r_+ - r_- \\approx \\frac{\\mathrm \\partial r(\\Theta)}{\\mathrm \\partial \\Theta)} $\n",
    "\n",
    "- as a result of the method of finite differences we got the following approximation\n",
    "\n",
    "$(r_+ - r_-)\\Delta \\approx \\frac{\\mathrm \\partial r(\\Theta)}{\\mathrm \\partial \\Theta)}d\\Theta $\n",
    "\n",
    "- we choose a number n of best directions we want to keep\n",
    "    - this is the directions leading to the highest rewards \n",
    "\n",
    "How do we know these directions?\n",
    "- we use all weights_plus_pos_delta and weights_plus_neg_delta for one full episode and store the resulting couples of rewards $(r_+, r_-)$ \n",
    "- we keep the directions with the highest n maximums of $(r_+, r_-)$ \n",
    "\n",
    "4.) Update Step\n",
    "- we calculate the approximated gradients of the n best directions and use it to update our original weight matrix\n",
    "    - the weights are updated in the top directions that increase the accumulated reward the most\n",
    "\n",
    "$\\Theta(new) = \\Theta(old) + \\frac{1}{n} \\sum_{k=1}^{n}[r_{weights\\_plus\\_pos\\_delta[k]} - r_{weights\\_plus\\_neg\\_delta[k]}] \\Delta_{[k]}$\n",
    "\n",
    "    \n",
    "5.) Training\n",
    "- we repeat the process (except the init phase) for a certain number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalizing the states improves performance\n",
    "- deviding the sum of our best directions by the standard deviation\n",
    "- for tuning purposes we can add a learning rate update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the learning process, the creatures tend to learn only to stand still cause they get a high amount of rewards just for staying alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Half Cheetah Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"./monitor/HalfCheetahBulletEnv-v0/openaigym.video.0.12196.video000000.mp4\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
